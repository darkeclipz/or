{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of maximizing a _concave_ function $f(\\mathbf{x})$ of _multiple_ variables $\\mathbf{x}=(x_1,x_2,\\ldots,x_n)$ where there are no constraints on the feasible values. A number of search procedures are available for solving such a problem numerically. One of these, the _gradient search procedure_, is an especially important one because it identifies and uses the direction of movement from the current trial solution that maximizes the rate at which $f(\\mathbf{x})$ is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Plotting and Typesetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will initialize the notebook, define a typesetting for plotting, and a function `plot3d` that creates plots in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\darky\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['solve', 'diff']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import *\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sympy import symbols, diff, solve, Matrix\n",
    "font = {'size': 14}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(f,lim=(-5,5),title='Surface plot',detail=0.05,path=[]):\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    xs = ys = np.arange(lim[0],lim[1], detail)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    zs = np.array([f(x, y) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
    "    Z = zs.reshape(X.shape)\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cm.Blues)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    xlabel('X-axis');ylabel('Y-axis');ax.set_zlabel('Z-axis')\n",
    "    plt.title(title);\n",
    "    if len(path):\n",
    "        xs, ys, zs = path[0], path[1], path[2]\n",
    "        ax.plot(xs,ys,zs, 'o', color='r', markersize=5, zorder=3)\n",
    "        for i in range(len(xs)-1):\n",
    "            ax.add_artist(Arrow3D([xs[i], xs[(i+1)%len(xs)]], \n",
    "                                  [ys[i], ys[(i+1)%len(xs)]], \n",
    "                                  [zs[i], zs[(i+1)%len(xs)]],\n",
    "                                  mutation_scale=15, lw=2, arrowstyle=\"-|>\", \n",
    "                                  color='r', zorder=4)) \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convexity/Concavity Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the Hessian for a function $f(\\mathbf{x})$ to determine if it is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(f):\n",
    "    x, y = symbols('x y')\n",
    "    f = f(x,y)\n",
    "    pf_px = f.diff(x)\n",
    "    pf_py = f.diff(y)\n",
    "    p2f_pxpy = pf_px.diff(y)\n",
    "    return Matrix([[pf_px.diff(x), p2f_pxpy], [p2f_pxpy, pf_py.diff(y)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the determinant of the Hessian is positive and $\\dfrac{\\partial f}{\\partial x \\partial y}$ is also positive, the function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_convex(f):\n",
    "    H = hessian(f)\n",
    "    return H.det() > 0 and H[0,0] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the determinant of the Hessian is positive and $\\dfrac{\\partial f}{\\partial x \\partial y}$ is negative, the function is concave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_concave(f):\n",
    "    H = hessian(f)\n",
    "    return H.det() > 0 and H[0,0] < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gradient Search Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single variable function, there are only two possible directions (increase $x$ or decrease $x$) in which to move from the current trial solution to the next one. The goal was to reach a point where eventually the derivative is (essentially) $0$. Now, there are _innumerable_ possible directions in which to move; they correspond to the possible _proportional rates_ at which the respective variables can be changed. The goal is to reach a point eventually where all the partial derivates are (essentially) $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "f = lambda x,y: 2*x*y + 2*y - x**2 - 2*y**2\n",
    "xs = np.linspace(-9,0,n)\n",
    "ys = np.linspace(-9,0,n)\n",
    "zs = f(xs,ys)\n",
    "plot3d(f, lim=(-10,10), path=[xs,ys,zs], \n",
    "       title='Theoretical Path to Global Maximum');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the objective function $f(\\mathbf{x})$ is assumed to be differentiable, it possesses a gradient, denoted by $\\nabla f(\\mathbf{x})$, at each point $\\mathbf{x}$. In particular, the **gradient** at a specific point $\\mathbf{x}=\\mathbf{x}'$ is the _vector_ whose elements are the respective _partial derivatives_ evaluated at $\\mathbf{x}=\\mathbf{x}'$, so that:\n",
    "\n",
    "$$ \\nabla f(\\mathbf{x}') = \\left( \\dfrac{\\partial f}{\\partial x_1}, \\dfrac{\\partial f}{\\partial x_2}, \\ldots, \\dfrac{\\partial f}{\\partial x_n} \\right) \\quad \\textrm{at } \\mathbf{x}=\\mathbf{x}'.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance of the gradient is that the (infinitesimal) change in $\\mathbf{x}$ that _maximizes_ the rate at which $f(\\mathbf{x})$ increases is the change _proportional_ to $\\nabla f(\\mathbf{x})$. It may be said that the rate at which $f(\\mathbf{x})$ increases is maximized if (infinitesimal) changes in $\\mathbf{x}$ are in the _direction_ of the gradient $\\nabla f(\\mathbf{x})$. Because the current problem has no constraints, this interpretation of the gradient suggests that an efficient search procedure should keep moving in the direction of the gradient until it (essentially) reaches an optimal solution $\\mathbf{x}^*$, where $\\nabla f(\\mathbf{x}^*)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the Gradient Search Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Add a summary of the gradient search procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following two-variable problem:\n",
    "\n",
    "$$ \\max f(\\mathbf{x}) = 2xy + 2y -x^2 -2y^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y: 2*x*y + 2*y - x**2 - 2*y**2\n",
    "lim=(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a surface plot of $f(\\mathbf{x})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(f, lim=lim, title='Surface plot of $f(\\\\mathbf{x})$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will check if $f(\\mathbf{x})$ is a concave function, for $f(\\mathbf{x})$ to be concave, the determinant of the Hessian of $f(\\mathbf{x})$ should be positive, and any second-order partial derivate $f(\\mathbf{x})_{xy}$ or $f(\\mathbf{x})_{yx}$ must be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[-2,  2],\n",
       "[ 2, -4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_concave(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $f(\\mathbf{x})$ we can find that $\\dfrac{\\partial f}{\\partial x} = 2y - 2x$ and $\\dfrac{\\partial f}{\\partial y} = 2x + 2 - 4y$. This gives us the gradient $\\nabla f(\\mathbf{x})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = lambda x: np.array([2*x[1] - 2*x[0], \n",
    "                           2*x[0] + 2 - 4*x[1]]) # gradient of f\n",
    "\n",
    "xp = np.array([0, 0]) # trial solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin the gradient search procedure, after choosing a suitably small value of $\\epsilon$ (normally well under $0.1$) suppose that $\\mathbf{x}=(0,0)$ is selected as the initial trial solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(f, lim=lim, path=[[0],[0],[f(0,0)]], \n",
    "       title='Initial Trial Solution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the respective partial derivatives are $0$ and $2$ at this point, the gradient is $\\nabla f(0,0) = (0,2)$. With $\\epsilon < 2$, the stopping rule then says to perform an iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 1:** With values of $0$ and $2$ for the respective partial derivatives, the first iteration begins by setting:\n",
    "\n",
    "$$ x = 0 + t(0) = 0 \\\\ y = 0 + t(2) = 2t $$\n",
    "\n",
    "and then substituting these expressions into $f(\\mathbf{x})$ to obtain\n",
    "\n",
    "$$ f(\\mathbf{x}' + t \\nabla f(\\mathbf{x}')) = f(0, 2t) = 4t - 8t^2.$$\n",
    "\n",
    "Now we need to find a maximum in a function of one-variable, we can do this by differentiating $4t - 8t^2$ which is $4-16t$, from this it follows that $t^*=\\dfrac{1}{4}$, so we reset: $\\mathbf{x}'=(0,0) + \\dfrac{1}{4}(0,2)=(0,\\dfrac{1}{2})$. This completes the iteration. For this new trial solution, the gradient is $\\nabla f\\left(0,\\dfrac{1}{2}\\right)=(1,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[ 0.   0.5] with gradient ([ 1.  0.])\n"
     ]
    }
   ],
   "source": [
    "xp = xp + (1/4) * grad(xp)\n",
    "print('x={} with gradient ({})'.format(xp, grad(xp)))\n",
    "plot3d(f, lim=lim, path=[[0, 0],[0, 1/2],[f(0,0),f(0,1/2)]], \n",
    "       title='Trial solution after first iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\epsilon < 1$, the stopping rule now says to perform another iteration.\n",
    "\n",
    "**Iteration 2**: This time we will do the iteration with help of _SymPy_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0*t, 0.500000000000000], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = symbols('t') # define a symbol t\n",
    "u = grad(xp) * t + xp # find gradient vector\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.500000000000000]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(f(u[0], u[1]).diff()) # substitute the gradient vector into f(x), \n",
    "                            # then differentiate and solve for t (max. point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $t^* = \\dfrac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[ 0.5  0.5] with gradient ([ 0.  1.])\n"
     ]
    }
   ],
   "source": [
    "xp = xp + (1/2) * grad(xp)\n",
    "print('x={} with gradient ({})'.format(xp, grad(xp)))\n",
    "plot3d(f, lim=lim, path=[[0, 0, .5],[0, 1/2, .5],\n",
    "                         [f(0,0),f(0,1/2),f(.5,.5)]], \n",
    "       title='Trial solution after second iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completed the second iteration. With a typically small value of $\\epsilon$, the procedure would now continue on to several more iterations in similar fashion. If we find the maximum analytically, we find that optimal value is at $(1,1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(f, lim=lim, path=[[0, 0, .5, 1],[0, 1/2, .5, 1],\n",
    "                         [f(0,0),f(0,1/2),f(.5,.5),f(1,1)]], \n",
    "       title='Optimal solution (solved analytically)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implementation of the gradient search procedure will find the optimum, assuming that the Hessian only consists of constants, and that $f$ is concave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2*x + 2*y, 2*x - 8*y + 2], dtype=object),\n",
       " array([0.0277662256954319, 0.00775386621814267], dtype=object),\n",
       " array([-0.426712619718003*t + 0.36313548455173,\n",
       "        1.52803757156163*t + 0.149779174692728], dtype=object),\n",
       " 0.116249853163058,\n",
       " array([0.313530205166688, 0.327413318014404], dtype=object),\n",
       " array([ 15.        ,  -9.41246916,   0.36313548,   0.31353021]),\n",
       " array([-90.        ,  -2.58011042,   0.14977917,   0.32741332]),\n",
       " array([ -3.55050000e+04,  -7.18122562e+01,   1.86736031e-01,\n",
       "          3.33035453e-01]),\n",
       " 97)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function with powers higher than two are not supported.\n",
    "f = lambda x1,x2: 2*x1*x2 + 2*x2 - x1**2 - 4*x2**2\n",
    "\n",
    "# check if the hessian is positive.\n",
    "if hessian(f).det() < 0: raise ValueError('negative hessian!')\n",
    "    \n",
    "# create a sympy equation from f\n",
    "x, y, t = symbols('x y t')\n",
    "f_alg = f(x,y)\n",
    "\n",
    "# plot limit\n",
    "lim = (-100,100)\n",
    "\n",
    "# max error\n",
    "eps = 1\n",
    "\n",
    "# initial trial solution x'\n",
    "xp = np.array([15, -90])\n",
    "\n",
    "# stores the path (xyz values) to plot\n",
    "xs = [xp[0]]; ys = [xp[1]]; zs =[f(xp[0], xp[1])]\n",
    "\n",
    "# get partial derivatives for calculating the gradient\n",
    "grad = np.array([f_alg.diff(x), f_alg.diff(y)])\n",
    "\n",
    "# calculate the gradient vector at x'\n",
    "u = np.array([g.subs(x,xp[0]).subs(y,xp[1]) for g in grad])\n",
    "\n",
    "# in case it takes forever\n",
    "bail = 100\n",
    "\n",
    "# keep iterating while the partials are not (near) zero.\n",
    "while (abs(u[0]) > eps or abs(u[1]) > eps) and bail > 0:\n",
    "    \n",
    "    d = xp + t * u # multiply t into the gradient vector\n",
    "    tp = solve(f(d[0], d[1]).diff(t))[0].evalf() # substitute the \n",
    "                                                 # gradient vector\n",
    "                                                 # into f, diff, and\n",
    "                                                 # solve for t.\n",
    "    xp = xp + tp*u  # set x' to the new x'\n",
    "    \n",
    "    # append path for plotting\n",
    "    xs.append(xp[0]); ys.append(xp[1]); zs.append(f(xp[0], xp[1]))\n",
    "    \n",
    "    # calculate the gradient at x'\n",
    "    u = np.array([g.subs(x,xp[0]).subs(y,xp[1]) for g in grad])\n",
    "    \n",
    "    bail -= 1 # in case it takes forever\n",
    "\n",
    "# force casting to float64\n",
    "xs = np.array(xs, dtype='float64')\n",
    "ys = np.array(ys, dtype='float64')\n",
    "zs = np.array(zs, dtype='float64')\n",
    "\n",
    "# plot as a surface with the path\n",
    "plot3d(f, lim=lim, path=[xs,ys,zs], \n",
    "       title='Gradient Search Procedure', detail=1)\n",
    "\n",
    "# show all the variables\n",
    "f, 'concave: {}'.format(is_concave(f)), f_alg, lim, xp, \n",
    "grad, u, d, tp, xp, xs, ys, zs, bail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Multivariable unconstrained optimization, _Introduction to Operations Research_, 10th ed., Hillier and Lieberman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: SymPy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, solve, Matrix\n",
    "from sympy import diff\n",
    "from sympy import solve\n",
    "from sympy import Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z,t = symbols('x y z t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3*t**2 + 2*t"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = f(t, t)\n",
    "expr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-t**2 + 2*t > 0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2*t - t**2\n",
    "a > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr.subs(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6*t + 2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_dt = diff(expr)\n",
    "d_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1/3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(d_dt, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(lambda x,y: 2*x*y**3 + 2*y**3 - x**6 - 2*y**4*x, lim=(-10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6*x**5 - 2*y**4 + 2*y**3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 2*x*y**3 + 2*y**3 - x**6 - 2*y**4*x\n",
    "diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sympy.core.add.Add"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8*x*y**3 + 6*x*y**2 + 6*y**2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(f, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(f):\n",
    "    x, y = symbols('x y')\n",
    "    f = f(x,y)\n",
    "    pf_px = f.diff(x)\n",
    "    pf_py = f.diff(y)\n",
    "    p2f_pxpy = pf_px.diff(y)\n",
    "    return Matrix([[pf_px.diff(x), p2f_pxpy], [p2f_pxpy, pf_py.diff(y)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-x**2 + 2*x*y - 2*y**2 + 2*y\n",
      "[0.500000000000000 0.500000000000000]\n",
      "[0.500000000000000 0.750000000000000]\n",
      "[0.750000000000000 0.750000000000000]\n",
      "[0.750000000000000 0.875000000000000]\n",
      "[0.875000000000000 0.875000000000000]\n",
      "[0.875000000000000 0.937500000000000]\n",
      "[0.937500000000000 0.937500000000000]\n",
      "[0.937500000000000 0.968750000000000]\n",
      "[0.968750000000000 0.968750000000000]\n",
      "[0.968750000000000 0.984375000000000]\n",
      "[0.984375000000000 0.984375000000000]\n",
      "[0.984375000000000 0.992187500000000]\n",
      "[0.992187500000000 0.992187500000000]\n",
      "[0.992187500000000 0.996093750000000]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x,y: 2*x*y + 2*y - x**2 - 2*y**2\n",
    "eps = 0.01\n",
    "x,y,t = symbols('x y t')\n",
    "f_alg = f(x,y)\n",
    "print(f_alg)\n",
    "xp = np.array([0,0.5])\n",
    "grad = np.array([f_alg.diff(x), f_alg.diff(y)])\n",
    "grad_at_p = np.array([g.subs(x, xp[0]).subs(y, xp[1]) for g in grad])\n",
    "gvec = xp + t * grad_at_p\n",
    "magnitude = solve(f(gvec[0], gvec[1]).diff(t))\n",
    "xp1 = xp + magnitude[0].evalf() * grad_at_p\n",
    "\n",
    "xp, grad, grad_at_p, gvec, magnitude, xp1\n",
    "while abs(grad_at_p[0]) >= eps or abs(grad_at_p[1]) >= eps:\n",
    "    gvec = xp + t * grad_at_p\n",
    "    magnitude = solve(f(gvec[0], gvec[1]).diff(t))\n",
    "    xp = xp + magnitude[0].evalf() * grad_at_p\n",
    "    print(xp)\n",
    "    grad_at_p = np.array([g.subs(x, xp[0]).subs(y, xp[1]) for g in grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
